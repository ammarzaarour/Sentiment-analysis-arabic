{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4mZ8KYblg-g"
      },
      "source": [
        "#installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate #makes it easy to train and use PyTorch models with multi-GPU, TPU, mixed-precision, and large models.\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "Uh7Ak5B4cLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4tlwQ8si_FI"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y024z5AnlTLz"
      },
      "source": [
        "\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "!pip install emoji\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f3KKtL5yMub"
      },
      "source": [
        "Let's download some Arabic text classification datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpdi0DENyLmt"
      },
      "source": [
        "!git clone https://github.com/elnagara/HARD-Arabic-Dataset\n",
        "!git clone https://github.com/mahmoudnabil/ASTD\n",
        "!wget http://homepages.inf.ed.ac.uk/wmagdy/Resources/ArSAS.zip\n",
        "!unzip ArSAS.zip\n",
        "!unrar x '/content/HARD-Arabic-Dataset/data/unbalanced-reviews.rar'\n",
        "!unzip '/content/HARD-Arabic-Dataset/data/balanced-reviews.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVay9KamnC3I"
      },
      "source": [
        "#Creating training datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr84ozGinCFh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jD0IueryovK"
      },
      "source": [
        "This custom dataset class will help us hold our datasets in a structred manner.\n",
        "It's not necessary to use it with your own data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PhWP2JzrEci"
      },
      "source": [
        "class CustomDataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        train: List[pd.DataFrame],\n",
        "        test: List[pd.DataFrame],\n",
        "        label_list: List[str],\n",
        "    ):\n",
        "        \"\"\"Class to hold and structure datasets.\n",
        "\n",
        "        Args:\n",
        "\n",
        "        name (str): holds the name of the dataset so we can select it later\n",
        "        train (List[pd.DataFrame]): holds training pandas dataframe with 2 columns [\"text\",\"label\"]\n",
        "        test (List[pd.DataFrame]): holds testing pandas dataframe with 2 columns [\"text\",\"label\"]\n",
        "        label_list (List[str]): holds the list  of labels\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WYy5ikAs7l3"
      },
      "source": [
        "# This will hold all the downloaded and structred datasets\n",
        "all_datasets= []\n",
        "DATA_COLUMN = \"text\"\n",
        "LABEL_COLUMN = \"label\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sHrgqmOzZ-4"
      },
      "source": [
        "You can choose which ever dataset you like or use your own.\n",
        "At this stage we don't do any preprocessing on the text, this is done later when loading the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joWPYWGMqLau"
      },
      "source": [
        "##HARD - Balanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6oL3qkXmOgJ"
      },
      "source": [
        "df_HARD = pd.read_csv(\"/content/balanced-reviews.txt\", sep=\"\\t\", header=0,encoding='utf-16') # loads the dataset into a Pandas DataFrame\n",
        "\n",
        "df_HARD = df_HARD[[\"review\",\"rating\"]]  # we are interested in rating and review only\n",
        "df_HARD.columns = [DATA_COLUMN, LABEL_COLUMN]#rename the columns with text and label assigned before\n",
        "print(df_HARD[LABEL_COLUMN].value_counts()) #Return a Series containing counts of unique values.\n",
        "# code rating as +ve if > 3, -ve if less, no 3s in dataset\n",
        "\n",
        "hard_map = {\n",
        "    5: 'POS',\n",
        "    4: 'POS',\n",
        "    2: 'NEG',\n",
        "    1: 'NEG'\n",
        "}\n",
        "\n",
        "df_HARD[LABEL_COLUMN] = df_HARD[LABEL_COLUMN].apply(lambda x: hard_map[x])#rating as positive if it is greater than 3 and negative if it is less than 3\n",
        "train_HARD, test_HARD = train_test_split(df_HARD, test_size=0.2, random_state=42)# split the dataset into training and test sets, random_state parameter is used to ensure that the split is reproducible\n",
        "label_list_HARD = ['NEG', 'POS'] #creates a list of labels for the HARD dataset\n",
        "\n",
        "data_Hard = CustomDataset(\"HARD\", train_HARD, test_HARD, label_list_HARD)#The CustomDataset object will store the training and test data for the HARD dataset, as well as the list of labels. This will make it easy to access and manage the data for the HARD dataset later on.\n",
        "all_datasets.append(data_Hard)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sycNXzvgr7BZ"
      },
      "source": [
        "##ASTD- Unbalanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wapHixOBrq5U"
      },
      "source": [
        "df_ASTD_UN = pd.read_csv(\n",
        "    \"/content/ASTD/data/Tweets.txt\", sep=\"\\t\", header=None\n",
        ")\n",
        "\n",
        "df_ASTD_UN.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "\n",
        "df_ASTD_UN = df_ASTD_UN[df_ASTD_UN[LABEL_COLUMN]!= 'OBJ']\n",
        "\n",
        "train_ASTD_UN, test_ASTD_UN = train_test_split(\n",
        "    df_ASTD_UN, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "label_list_ASTD_UN = list(df_ASTD_UN[LABEL_COLUMN].unique())\n",
        "print(label_list_ASTD_UN)\n",
        "print(df_ASTD_UN[LABEL_COLUMN].value_counts())\n",
        "\n",
        "data_ASTD_UN = CustomDataset(\n",
        "    \"ASTD-Unbalanced\", train_ASTD_UN, test_ASTD_UN, label_list_ASTD_UN\n",
        ")\n",
        "\n",
        "all_datasets.append(data_ASTD_UN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA5607R9jU3a"
      },
      "source": [
        "##AJGT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CJA2XSfo2D8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bww9o0i7uQSd"
      },
      "source": [
        "df_AJGT = pd.read_excel(\"/content/drive/My Drive/Datasets/AJGT.xlsx\", header=0)\n",
        "\n",
        "df_AJGT = df_AJGT[[\"Feed\", \"Sentiment\"]]\n",
        "df_AJGT.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "\n",
        "\n",
        "\n",
        "train_AJGT, test_AJGT = train_test_split(df_AJGT, test_size=0.2, random_state=42)\n",
        "\n",
        "print(df_AJGT[LABEL_COLUMN].value_counts())\n",
        "label_list_AJGT = list(df_AJGT[LABEL_COLUMN].unique())\n",
        "\n",
        "data_AJGT = CustomDataset(\"AJGT\", train_AJGT, test_AJGT, label_list_AJGT)\n",
        "all_datasets.append(data_AJGT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFYs8DEEvHoX"
      },
      "source": [
        "for x in all_datasets:\n",
        "  print(x.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blL_Z_oQ2gY9"
      },
      "source": [
        "## ArSAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOiRKNPy2wFL"
      },
      "source": [
        "df_ArSAS = pd.read_csv(\"/content/ArSAS..txt\", sep=\"\\t\",encoding='utf-8')\n",
        "df_ArSAS = df_ArSAS[[\"Tweet_text\",\"Sentiment_label\"]]  # we are interested in rating and review only\n",
        "df_ArSAS.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "print(\"Total length: \", len(df_ArSAS))\n",
        "print(df_ArSAS[LABEL_COLUMN].value_counts())\n",
        "\n",
        "label_list_ArSAS = list(df_ArSAS[LABEL_COLUMN].unique())\n",
        "print(label_list_ArSAS)\n",
        "\n",
        "train_ArSAS, test_ArSAS = train_test_split(df_ArSAS, test_size=0.2, random_state=42)\n",
        "print(\"Training length: \", len(train_ArSAS))\n",
        "print(\"Testing length: \", len(test_ArSAS))\n",
        "data_ArSAS = CustomDataset(\"ArSAS\", train_ArSAS, test_ArSAS, label_list_ArSAS)\n",
        "all_datasets.append(data_ArSAS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcwdslw7v0Q8"
      },
      "source": [
        "#Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhcYKB663eoz"
      },
      "source": [
        "Start the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUn2RB6Bvrxj"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                             confusion_matrix, f1_score, precision_score,\n",
        "                             recall_score)\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, BertTokenizer, Trainer,\n",
        "                          TrainingArguments)\n",
        "from transformers.data.processors.utils import InputFeatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tv-ZwCt3iZE"
      },
      "source": [
        "List all the datasets we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4SGYoB2EDJD"
      },
      "source": [
        "for x in all_datasets:\n",
        "  print(x.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzeVFoz1wDYf"
      },
      "source": [
        "# select a dataset\n",
        "dataset_name = 'AJGT'\n",
        "# select a model from the huggingface modelhub https://huggingface.co/models?language=ar\n",
        "model_name = 'UBC-NLP/ARBERT'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ieCOj90aw8X"
      },
      "source": [
        "for d in all_datasets:\n",
        "  if d.name==dataset_name:\n",
        "    selected_dataset = copy.deepcopy(d) #copies the CustomDataset object\n",
        "    print('Dataset found')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icuhwx3S4NLM"
      },
      "source": [
        "Create and apply preprocessing using the AraBERT processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt_lGy85zuca"
      },
      "source": [
        "arabic_prep = ArabertPreprocessor(model_name)\n",
        "\n",
        "selected_dataset.train[DATA_COLUMN] = selected_dataset.train[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))\n",
        "selected_dataset.test[DATA_COLUMN] = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-tHPrqeEqHm"
      },
      "source": [
        "# Sanity check on the dataset\n",
        "list(selected_dataset.train[DATA_COLUMN][0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH2yldN-4vcX"
      },
      "source": [
        "Now we need to check the tokenized sentence length to decide on the maximum sentence length value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOVdVmfEguAT"
      },
      "source": [
        "tok = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP5ziUabgpeb"
      },
      "source": [
        "print(\"Training Sentence Lengths: \")\n",
        "plt.hist([ len(tok.tokenize(sentence)) for sentence in selected_dataset.train[DATA_COLUMN].to_list()],bins=range(0,128,2))\n",
        "plt.show()\n",
        "\n",
        "print(\"Testing Sentence Lengths: \")\n",
        "plt.hist([ len(tok.tokenize(sentence)) for sentence in selected_dataset.test[DATA_COLUMN].to_list()],bins=range(0,128,2))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSDt3uyd5K2C"
      },
      "source": [
        "Let's select 128 as our maximum sentence length, and check how many sequences will be truncated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09RumWom5bG2"
      },
      "source": [
        "max_len = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLhNqO2w5DIn"
      },
      "source": [
        "print(\"Truncated training sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in selected_dataset.test[DATA_COLUMN].to_list()]))\n",
        "\n",
        "print(\"Truncated testing sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in selected_dataset.test[DATA_COLUMN].to_list()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqENWrht6A5n"
      },
      "source": [
        "Now let's create a classification dataset to load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YS7XI2bZTyz"
      },
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(ClassificationDataset).__init__()\n",
        "      \"\"\"\n",
        "      Args:\n",
        "      text (List[str]): List of the training text\n",
        "      target (List[str]): List of the training labels\n",
        "      tokenizer_name (str): The tokenizer name (same as model_name).\n",
        "      max_len (int): Maximum sentence length\n",
        "      label_map (Dict[str,int]): A dictionary that maps the class labels to integer\n",
        "      \"\"\"\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "      inputs = self.tokenizer(\n",
        "          text,\n",
        "          max_length=self.max_len,\n",
        "          padding='max_length',\n",
        "          truncation=True\n",
        "      )\n",
        "      return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mciZOFz-amkV"
      },
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
        "print(label_map)\n",
        "\n",
        "train_dataset = ClassificationDataset(\n",
        "    selected_dataset.train[DATA_COLUMN].to_list(),\n",
        "    selected_dataset.train[LABEL_COLUMN].to_list(),\n",
        "    model_name,\n",
        "    max_len,\n",
        "    label_map\n",
        "  )\n",
        "test_dataset = ClassificationDataset(\n",
        "    selected_dataset.test[DATA_COLUMN].to_list(),\n",
        "    selected_dataset.test[LABEL_COLUMN].to_list(),\n",
        "    model_name,\n",
        "    max_len,\n",
        "    label_map\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoOyOzRu8Jwo"
      },
      "source": [
        "Check the dataset output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhi41co270I9"
      },
      "source": [
        "print(next(iter(train_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFHzbq6p8P2C"
      },
      "source": [
        "Create a function that return a pretrained model ready to do classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt7l0IxjbmNu"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WCYBfiB8hIb"
      },
      "source": [
        "Define whatever metric you want here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYU6G4vWc5nz"
      },
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  assert len(preds) == len(p.label_ids)\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "  acc = accuracy_score(p.label_ids,preds)\n",
        "  return {\n",
        "      'macro_f1' : macro_f1,\n",
        "      'accuracy': acc\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEznOOD4COn7"
      },
      "source": [
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic=True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTmvFEs41WkV"
      },
      "source": [
        "#Regular Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hym-1aiJ8zS8"
      },
      "source": [
        "Define our training parameters.\n",
        "Check the TrainingArguments documentation for more options https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BplVarm6h6qj"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir= \"./train\",\n",
        "\n",
        "\n",
        "\n",
        "    evaluation_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "\n",
        "    seed = 25\n",
        "  )\n",
        "\n",
        "set_seed(training_args.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVODSK3D89OZ"
      },
      "source": [
        "Create the trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro5BW5ak4uc1"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = model_init(),\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxqoZsFWSFso"
      },
      "source": [
        "#start the training\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZJE_H0j-ldm"
      },
      "source": [
        "Save the model, the tokenizer and the config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pru84em-lMR"
      },
      "source": [
        "inv_label_map = inv_label_map = { v:k for k, v in label_map.items()}\n",
        "print(inv_label_map)\n",
        "trainer.model.config.label2id = label_map\n",
        "trainer.model.config.id2label = inv_label_map\n",
        "trainer.save_model(\"output_dir\")\n",
        "train_dataset.tokenizer.save_pretrained(\"output_dir\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckb8jYZu_pCu"
      },
      "source": [
        "#copy the model to drive\n",
        "!cp output_dir /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaDLMebgF8EE"
      },
      "source": [
        "## predict using the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFqMbH5uF-TR"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4k9zqkF7Wb"
      },
      "source": [
        "# initialize pipline\n",
        "pipe = pipeline(\"sentiment-analysis\", model=\"output_dir\", device=0, return_all_scores=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ecQR0_tGDQ0"
      },
      "source": [
        "pipe(\"مقرف\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}